,07.07.25,08.07.25,08.07.25,09.07.25,,
Model,M1 bert-base-uncased,M2 bert-case-uncased,M3 bertimbau_hate_speech,M4  distilbert-portuguese-cased,,
Description,CE only,CE only,CE only,CE only,,
Epochs,3,3,3,3,,
Alpha,0,0,0,0,,
Best Val F1,0.8495,0.7856,0.9295,0.9028,,
Test F1,0.8533,0.8036,0.9256,0.9018,,
Precision ,0.8534,0.8046,0.9264,0.9023,,
Recall,0.8533,0.8038,0.9257,0.9019,,
Val Loss,0.3916,0.4823,0.2314,0.2789,,
 Attention-Rationale Alignment Score,,,,,,
,Max length = 512,Max length = 512,Max length = 512,Max length = 512,,
,Batch size = 16,Batch size = 16,Batch size = 16,Batch size = 16,,
Note: Split is 70/15/15 not 80/10/10,,,,,,
,,,,,,
Per- class performance,,,,,,
525 samples in each class,,,,,,
,M1 Precision,M1 Recall,M1 F1,M2 Precision,M2 Recall,M2 F2
Offensive,0.8601,0.8438,0.8519,0.8209,0.7771,0.7984
Non-Offensive,0.8467,0.8628,0.8547,0.7884,0.8304,0.8089
,,,,,,
,M3 Precision,M3 Recall,M3 F1,M4 Precision,M4 Recall,M4 F1
Offensive,0.9443,0.9047,0.9241,0.9153,0.8857,0.9002
Non-Offensive,0.9085,0.9466,0.9272,0.8892,0.918,0.9034
,,,,,,
,,,,,,
Other hyperparameters,,,,,,
Batch size,16,16,16,16,,
LR,2.00E-05,5.00E-06,2.00E-05,2.00E-05,,
Warmup steps,500,200,500,500,,
Weight decay,0.01,0.01,0.01,0.01,,
G_A_S,1,1,1,1,,
Max lenght,512,512,512,512,,
Dropout rate,0.1,0.1,0.1,0.1,,
,,,,,,
Other SRA paremters,,,,,,
Attention layer, -1 for last layer only,,,,,
Attention head,"none, average across heads",,,,,
Rationale threshold,0.5,,,,,
